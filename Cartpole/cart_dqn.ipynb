{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "import os\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 15:03:24.074662: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-07 15:03:24.107112: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 15:03:24.107193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 15:03:24.107962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-07 15:03:24.115416: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 15:03:24.718837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step:\n",
      "TimeStep(\n",
      "{'step_type': array(0, dtype=int32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.00998398,  0.01919768,  0.0427773 ,  0.04371807], dtype=float32)})\n",
      "Next time step:\n",
      "TimeStep(\n",
      "{'step_type': array(1, dtype=int32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.00960003,  0.21368094,  0.04365166, -0.23516726], dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = np.array(1, dtype=np.int32)\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 15:03:26.012806: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.046807: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.046860: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.051586: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.051644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.051663: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.151799: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.151860: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.151866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-07 15:03:26.151891: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-07 15:03:26.151906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-04-07 15:03:26.381344: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_environment = tf_py_environment.TFPyEnvironment(\n",
    "    suite_gym.load('CartPole-v0'))\n",
    "time_step = example_environment.reset()\n",
    "random_policy.action(time_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmp6w5j_vtc.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:565] Loading latest checkpoint from /tmp/tmp6w5j_vtc\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 46317\n"
     ]
    }
   ],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)),\n",
       " 'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
       " 'policy_info': (),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.collect_data_spec._fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TimeStep(\n",
       " {'step_type': array(1, dtype=int32),\n",
       "  'reward': array(1., dtype=float32),\n",
       "  'discount': array(1., dtype=float32),\n",
       "  'observation': array([ 0.0984876 ,  0.37752202, -0.1401656 , -0.7647675 ], dtype=float32)}),\n",
       " ())"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(train_py_env.reset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(Trajectory(\n",
       "{'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(64, 2, 4), dtype=tf.float32, name=None),\n",
       " 'action': TensorSpec(shape=(64, 2), dtype=tf.int64, name=None),\n",
       " 'policy_info': (),\n",
       " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fab8f91b310>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "print(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (405180) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (405180) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (405180) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (405180) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (405180) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (405180) so Table uniform_table is accessed directly without gRPC.\n",
      "2024-04-07 15:07:02.488926: I external/local_xla/xla/service/service.cc:168] XLA service 0x7faa4c20e830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-07 15:07:02.488968: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-04-07 15:07:02.500051: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-07 15:07:02.537458: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712482622.603258  405296 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 26.518783569335938\n",
      "step = 400: loss = 18.981246948242188\n",
      "step = 600: loss = 128.36273193359375\n",
      "step = 800: loss = 17.12343978881836\n",
      "step = 1000: loss = 15.037732124328613\n",
      "step = 1000: Average Return = 117.30000305175781\n",
      "step = 1200: loss = 130.9974365234375\n",
      "step = 1400: loss = 1212.3201904296875\n",
      "step = 1600: loss = 39.48508071899414\n",
      "step = 1800: loss = 96.6206283569336\n",
      "step = 2000: loss = 15.066208839416504\n",
      "step = 2000: Average Return = 191.8000030517578\n",
      "step = 2200: loss = 19.351627349853516\n",
      "step = 2400: loss = 1029.9332275390625\n",
      "step = 2600: loss = 104.63630676269531\n",
      "step = 2800: loss = 57.696231842041016\n",
      "step = 3000: loss = 182.16903686523438\n",
      "step = 3000: Average Return = 165.60000610351562\n",
      "step = 3200: loss = 80.94483947753906\n",
      "step = 3400: loss = 51.411434173583984\n",
      "step = 3600: loss = 832.75146484375\n",
      "step = 3800: loss = 103.17930603027344\n",
      "step = 4000: loss = 131.71633911132812\n",
      "step = 4000: Average Return = 177.6999969482422\n",
      "step = 4200: loss = 58.276222229003906\n",
      "step = 4400: loss = 6371.22802734375\n",
      "step = 4600: loss = 108.49526977539062\n",
      "step = 4800: loss = 151.778564453125\n",
      "step = 5000: loss = 161.74148559570312\n",
      "step = 5000: Average Return = 162.1999969482422\n",
      "step = 5200: loss = 111.78343963623047\n",
      "step = 5400: loss = 86.66465759277344\n",
      "step = 5600: loss = 125.67587280273438\n",
      "step = 5800: loss = 60.82911682128906\n",
      "step = 6000: loss = 57.592529296875\n",
      "step = 6000: Average Return = 151.3000030517578\n",
      "step = 6200: loss = 99.546142578125\n",
      "step = 6400: loss = 40.36330032348633\n",
      "step = 6600: loss = 51.51036834716797\n",
      "step = 6800: loss = 109.5951919555664\n",
      "step = 7000: loss = 36.53982162475586\n",
      "step = 7000: Average Return = 193.10000610351562\n",
      "step = 7200: loss = 128.9795684814453\n",
      "step = 7400: loss = 48.1798095703125\n",
      "step = 7600: loss = 165.91415405273438\n",
      "step = 7800: loss = 33.62162780761719\n",
      "step = 8000: loss = 48.93276596069336\n",
      "step = 8000: Average Return = 200.0\n",
      "step = 8200: loss = 1012.29248046875\n",
      "step = 8400: loss = 55.025917053222656\n",
      "step = 8600: loss = 41.9549560546875\n",
      "step = 8800: loss = 72.41908264160156\n",
      "step = 9000: loss = 219.04165649414062\n",
      "step = 9000: Average Return = 200.0\n",
      "step = 9200: loss = 89.98617553710938\n",
      "step = 9400: loss = 59.27511978149414\n",
      "step = 9600: loss = 5603.85107421875\n",
      "step = 9800: loss = 10654.2763671875\n",
      "step = 10000: loss = 193.9551239013672\n",
      "step = 10000: Average Return = 200.0\n",
      "step = 10200: loss = 3028.822998046875\n",
      "step = 10400: loss = 252.49053955078125\n",
      "step = 10600: loss = 148.95484924316406\n",
      "step = 10800: loss = 194.1905517578125\n",
      "step = 11000: loss = 221.0153350830078\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 224.4238739013672\n",
      "step = 11400: loss = 159.74093627929688\n",
      "step = 11600: loss = 207.71665954589844\n",
      "step = 11800: loss = 141.7915496826172\n",
      "step = 12000: loss = 278.9394836425781\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 310.2258605957031\n",
      "step = 12400: loss = 108.52793884277344\n",
      "step = 12600: loss = 134.47377014160156\n",
      "step = 12800: loss = 410.6751708984375\n",
      "step = 13000: loss = 142.16455078125\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 304.67205810546875\n",
      "step = 13400: loss = 332.95703125\n",
      "step = 13600: loss = 8045.328125\n",
      "step = 13800: loss = 276.0773010253906\n",
      "step = 14000: loss = 20829.919921875\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 162.48684692382812\n",
      "step = 14400: loss = 13807.361328125\n",
      "step = 14600: loss = 13033.1044921875\n",
      "step = 14800: loss = 490.9383544921875\n",
      "step = 15000: loss = 333.2525634765625\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 336.9629211425781\n",
      "step = 15400: loss = 572.6489868164062\n",
      "step = 15600: loss = 167.961669921875\n",
      "step = 15800: loss = 320.986083984375\n",
      "step = 16000: loss = 234.10708618164062\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 436.1049499511719\n",
      "step = 16400: loss = 437.12249755859375\n",
      "step = 16600: loss = 510.2071838378906\n",
      "step = 16800: loss = 734.3338012695312\n",
      "step = 17000: loss = 571.2919311523438\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 564.2496948242188\n",
      "step = 17400: loss = 361.85675048828125\n",
      "step = 17600: loss = 68983.953125\n",
      "step = 17800: loss = 77879.5078125\n",
      "step = 18000: loss = 54373.9453125\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 243.0559539794922\n",
      "step = 18400: loss = 566.4945678710938\n",
      "step = 18600: loss = 753.5260009765625\n",
      "step = 18800: loss = 532.6199951171875\n",
      "step = 19000: loss = 449.211181640625\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_py_env.reset()\n",
    "\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps and save to the replay buffer.\n",
    "  time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
